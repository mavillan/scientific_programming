{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Scientific Programming in Python</h1>\n",
    "<h2 align=\"center\"> Topic 8: Basics of Parallelism </h2> \n",
    "\n",
    "\n",
    "_Notebook created by Mart√≠n Villanueva - `martin.villanueva@usm.cl` - DI UTFSM - June 2017._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Since this notebook makes use of Ipython Interactive widgets, you have to run the corresponding cell in order to visualize it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='about' />\n",
    "## 1.- About Parallelism in Python\n",
    "\n",
    "Some general concepts:\n",
    "\n",
    "1. __Concurrency:__ Concurrency is concerned with managing access to shared state from different threads.\n",
    "2. __Parallelism:__  Parallelism is concerned with utilizing multiple processors/cores to improve the performance of a computation.\n",
    "3. __Process:__  __A process is an instance of a computer program that is being executed.__  A process may be made up of multiple threads of execution that execute instructions concurrently.\n",
    "4. __Threads:__ A thread is an execution context, which is all the information a CPU needs to execute a stream of instructions.\n",
    "\n",
    "\n",
    "\n",
    "### Threads vs Processes\n",
    "\n",
    "First thing you need to know to understand the difference between a process and a thread, is a fact, __that processes do not run, threads do.__\n",
    "\n",
    "* __Process__:\n",
    "Each process provides the resources needed to execute a program. A process has a virtual address space, executable code, open handles to system objects, a security context, a unique process identifier, environment variables, a priority class, minimum and maximum working set sizes, and at least one thread of execution. Each process is started with a single thread, often called the primary thread, but can create additional threads from any of its threads.\n",
    "\n",
    "* __Thread__:\n",
    "A thread is an entity within a process that can be scheduled for execution. All threads of a process share its virtual address space and system resources. In addition, each thread maintains exception handlers, a scheduling priority, thread local storage, a unique thread identifier, and a set of structures the system will use to save the thread context until it is scheduled. The thread context includes the thread's set of machine registers, the kernel stack, a thread environment block, and a user stack in the address space of the thread's process. Threads can also have their own security context, which can be used for impersonating clients.\n",
    "\n",
    "<img src='data/threads.jpg' style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "### Global Interpreter Lock (GIL) in Python\n",
    "\n",
    "__From Python Documentation__: _In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython's memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)_\n",
    "\n",
    "The GIL is controversial because it __prevents multithreaded CPython programs from taking full advantage of multiprocessor systems__ in certain situations. Note that potentially blocking or long-running operations, such as I/O, image processing, and NumPy number crunching, __happen outside the GIL__. Therefore it is only in multithreaded programs that spend a lot of time inside the GIL, interpreting CPython bytecode, that the GIL becomes a bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='threading' />\n",
    "## 2.- Multithreading with `threading` and `numba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can completely bypass the GIL of Python with this two libraries: __`Numba`__ and __`Threading`__. The first (that we study  in a previuos session) allows us to create a _just in time_ compiled function, with a __`nogil`__ option that completely avoid any call to the CPython API in order to bypass the GIL. The second (__`Threading`__) allows us to create and handle Threads in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example we want to evaluate the function $f(x,y) = \\log(\\exp(x)\\cdot \\log(y))$ in a vectorial way, over two __huge__ unidimensional arrays $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^N$. \n",
    "\n",
    "This problem can be easily solved with __NumPy__ vectorized operations, but since we are working with huge arrays, we want to split the computation over such arrays into computation of __chunks__ of these arrays, each one performed by a single Thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit('void(double[:], double[:], int64, int64, double[:])', nopython=True, nogil=True)\n",
    "def numba_f(x, y, s_index, delta, res):\n",
    "    \"\"\"\n",
    "    x: (1d ndarray)\n",
    "    y: (1d ndarray)\n",
    "    s_index: (int) starting index\n",
    "    delta: (int) size of the chunk to compute\n",
    "    res: (1d ndarray) which store the results\n",
    "    \"\"\"\n",
    "    for i in range(s_index, min(s_index+delta, len(x))):\n",
    "        res[i] = np.log(np.exp(x[i]) * np.log(y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the huge unidimensional arrays to be passed to this function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data size, 100 million items\n",
    "N = int(100e6)\n",
    "\n",
    "# data\n",
    "x = np.random.random(N)\n",
    "y = np.random.random(N)\n",
    "\n",
    "# array for results\n",
    "r = np.zeros(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure the number of threads to use, the chunk size of each thread and the starting index of each thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of threads\n",
    "T = 4\n",
    "\n",
    "# data size for each thread\n",
    "chunk_size = N // T\n",
    "\n",
    "# starting index for each thread\n",
    "s_indexes = [i*chunk_size for i in range(T)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the threads, launch its execution, and wait for each one to finish its computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# threads creation\n",
    "threads = []\n",
    "for s_index in s_indexes:\n",
    "    threads.append( threading.Thread( target=numba_f, args=(x, y, s_index, chunk_size, r) ) )\n",
    "# threads start execution\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "# we wait for each thread to finish its execution\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "# all threads have finished here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can run a 1-threaded version as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_f(x, y, 0, N, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can benchmark the execution of both version with `timeit` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.09 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "threads = []\n",
    "for s_index in s_indexes:\n",
    "    threads.append( threading.Thread( target=numba_f, args=(x, y, s_index, chunk_size, r) ) )\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2372.86 MiB, increment: 0.05 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "threads = []\n",
    "for s_index in s_indexes:\n",
    "    threads.append( threading.Thread( target=numba_f, args=(x, y, s_index, chunk_size, r) ) )\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.36 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "numba_f(x, y, 0, N, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The creation of threads was included in the `timeit` calculation in order to make a fair comparison. \n",
    "* You can observe an improvement slightly higher than `x2` in the threaded version. __Why the improvement isn't `x4` if we are using $4$ threads?__ (Consider this machine which has $2$ physicall cores with __hyperthreading__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='multiprocessing' />\n",
    "## 3.- Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively if we are not worried about memory consumption due to the replication of resources caused by the creation of process, we can use the __`multiprocessing`__ library instead. It has the same exact sintax as the __`Threading`__ library with minor changes, but processes are created instead of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same example used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of processes\n",
    "jobs = []\n",
    "for s_index in s_indexes:\n",
    "    jobs.append( multiprocessing.Process( target=numba_f, args=(x, y, s_index, chunk_size, r) ) )\n",
    "# start execution of processes\n",
    "for job in jobs:\n",
    "    job.start()\n",
    "# we wait for each process to finish its execution\n",
    "for job in jobs:\n",
    "    job.join()\n",
    "# all processes have finished here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also perform a time benchmark with `timeit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.8 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "jobs = []\n",
    "for s_index in s_indexes:\n",
    "    jobs.append( multiprocessing.Process( target=numba_f, args=(x, y, s_index, chunk_size, r) ) )\n",
    "for job in jobs:\n",
    "    job.start()\n",
    "for job in jobs:\n",
    "    job.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the performance is not as good as with the __`Threading`__ library, but anyway there is an improvement to the naive/sequential version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2372.84 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "jobs = []\n",
    "for s_index in s_indexes:\n",
    "    jobs.append( multiprocessing.Process( target=numba_f, args=(x, y, s_index, chunk_size, r) ) )\n",
    "for job in jobs:\n",
    "    job.start()\n",
    "for job in jobs:\n",
    "    job.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='parallel' />\n",
    "## 4.- `IPython.Parallel`\n",
    "\n",
    "We will see how to run multiple tasks in parallel on a multicore computer. IPython implements highly-powerful and user-friendly facilities for interactive parallel computing in the Notebook.\n",
    "\n",
    "We first need to install `ipyparallel` (also called `IPython.parallel`) and configure it:\n",
    "```Bash\n",
    "    conda install ipyparallel\n",
    "```\n",
    "Then you have to add the line `c.NotebookApp.server_extensions.append('ipyparallel. nbextension')` to your configuration file: `~/.jupyter/jupyter_notebook_config.py`. If this file doesn't exist you must create it with:\n",
    "```Bash\n",
    "    jupyter notebook --generate-config\n",
    "```\n",
    "Finally, to enable the IPython Clusters tab in Jupyter Notebook run:\n",
    "```Bash\n",
    "    ipcluster nbextension enable\n",
    "```\n",
    "\n",
    "To use `IPython.parallel`, we need to launch a few engines. There are two ways:\n",
    "1. The  first way to do it is to run __`ipcluster start -n 4`__ in the terminal.\n",
    "2. Or you can also launch engines from the Notebook dashboard.\n",
    "\n",
    "__In general, you can launch as many engines as the number of CPUs you have on your machine.__\n",
    "\n",
    "__For more info:__ [Ipython Github Repository](https://github.com/ipython/ipyparallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipyparallel import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps to distribute code across multiple cores:\n",
    "1. Launching several IPython engines (there is typically one process per core).\n",
    "2. Creating a Client that acts as a __proxy to these engines__.\n",
    "3. Using the client to launch tasks on the engines and retrieve the results.\n",
    "\n",
    "Engines are __Python processes__ that execute code on different computing units. Once the engines have been launched, we create a Client instance that will give us access to these engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to access the engines:\n",
    "\n",
    "* With the __direct interface__, we have a direct access to every engine.\n",
    "* With the __load-balanced interface__, we submit jobs to a _scheduler_ which dynamically assigns them to the engines depending on their current load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Interface\n",
    "\n",
    "The `ids` attribute of the client shows us the identifiers of the engines that were automatically detected by `IPython`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to run code in parallel on the engines. First, we can use the `%px`\n",
    " magic command. \n",
    " \n",
    " The code passed to the `%px` magic command is executed on all engines. Here, we display the __OS process identifier__ (also called __PID__) of every engine. Every engine is an __independent Python process__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%px import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 15544\n",
      "[stdout:1] 15542\n",
      "[stdout:2] 15543\n",
      "[stdout:3] 15545\n"
     ]
    }
   ],
   "source": [
    "%px print(os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the exact list of engines to run code on. The --targets option accepts a list of engine identifiers.\n",
    "\n",
    "Note that we used the __cell magic__ `%%px` this time instead of the line magic. The cell magic allows us to execute several lines of code on the engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 15544\n",
      "[stdout:1] 15542\n",
      "[stdout:2] 15543\n"
     ]
    }
   ],
   "source": [
    "%%px --targets 0:3\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `%px` magic executes commands in __blocking mode__; the cell only returns when the commands have completed on all engines. It is possible to run __non-blocking commands__ with the __`--noblock`__ or __`-a`__ option. In this case, the cell __returns immediately__, and the task's status and results __can be polled asynchronously__ from IPython's interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: execute>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%px -a\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.276527 False\n"
     ]
    }
   ],
   "source": [
    "print(_.elapsed, _.ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __`%pxresult`__ blocks until the task finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.027085 True\n"
     ]
    }
   ],
   "source": [
    "%pxresult\n",
    "print(_.elapsed, _.ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a direct view on some or all of the engines. `IPython` provides convenient functions for common use cases, such as a __parallel `map` function__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirectView [0, 1, 2, 3]>\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "view = rc[:]\n",
    "print(view)\n",
    "res = view.map(lambda x: x*x, range(10))\n",
    "print(res.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direct view has a few useful methods like parallel versions of map() and apply(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-balanced Interface\n",
    "\n",
    "The load-balanced interface gives us high-level parallel computing routines that are dynamically executed on the engines.\n",
    "\n",
    "Here, we will demonstrate how to use it with a very known example: Estimate $\\pi$ in parallel using a __Monte-Carlo method__:\n",
    "* The __\"Monte Carlo Method\"__ is a method of solving problems using statistics. Given the probability, $P$, that an event will occur in certain conditions, a computer can be used to generate those conditions repeatedly. The number of times the event occurs divided by the number of times the conditions are generated should be approximately equal to $P$.\n",
    "* We will sample a large number of points uniformly in a __unit square__, and estimate the proportion of those which are in a __quarter unit circle__. We'll then get an estimation of pi since we know that this proportion should be pi/4.\n",
    "\n",
    "<img src='data/montecarlopi.png' style=\"width: 350px;\">\n",
    "\n",
    "\n",
    "Let's  first create a balanced view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function samples and counts the number of (_random_) points in the quarter disc. The second function below returns an estimation of $\\pi$ based on the number of points in the quarter disc, and the total number of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(n):\n",
    "    import numpy as np\n",
    "    # Random coordinates.\n",
    "    x, y = np.random.rand(2, n)\n",
    "    # Square distances to the origin.\n",
    "    r_square = x ** 2 + y ** 2\n",
    "    # Number of points in the quarter disc.\n",
    "    return (r_square <= 1).sum()\n",
    "\n",
    "def pi(n_in, n):\n",
    "    return 4. * float(n_in) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1414034"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000000\n",
    "pi(sample(n), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the time taken by this function on a single core:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 5.78 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit pi(sample(n),n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run this simulation in parallel. First, we divide this task into __100 smaller subtasks__ where the number of points is divided by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]\n"
     ]
    }
   ],
   "source": [
    "args = [n // 100] * 100\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a parallel `map()` function to run these tasks in parallel. Our `sample()` function is called 100 times, taking `n // 100` as its argument every time. We will combine the 100 results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar = v.map(sample, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a __synchronous__ version of `map()` called `map_sync()` which __blocks__ until the tasks have completed, and directly returns the results.\n",
    "\n",
    "This function doesn't return the results. Instead, it launches the 100 tasks in parallel and returns an __`AsyncResult`__ object. The __`AsyncResult`__ object can be used to interactively poll the tasks status and eventually retrieve the results. This object has a __`metadata`__ attribute: a list of dictionaries for all engines with useful information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'after': [],\n",
       " 'completed': datetime.datetime(2017, 6, 11, 18, 59, 2, 266188, tzinfo=tzutc()),\n",
       " 'data': {},\n",
       " 'engine_id': 1,\n",
       " 'engine_uuid': '149d4f71-b08ce1e3bfe43739aca00f86',\n",
       " 'error': None,\n",
       " 'execute_input': None,\n",
       " 'execute_result': None,\n",
       " 'follow': [],\n",
       " 'msg_id': 'a67a050e-f6e42d198c4047fc88d7c99f',\n",
       " 'outputs': [],\n",
       " 'received': datetime.datetime(2017, 6, 11, 18, 59, 2, 293158, tzinfo=tzutc()),\n",
       " 'started': datetime.datetime(2017, 6, 11, 18, 59, 2, 98913, tzinfo=tzutc()),\n",
       " 'status': 'ok',\n",
       " 'stderr': '',\n",
       " 'stdout': '',\n",
       " 'submitted': datetime.datetime(2017, 6, 11, 18, 59, 2, 83549, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ar.metadata))\n",
    "ar.metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 20)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.ready(), ar.progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the tasks are still running at this point, and that `ar.progress` tasks have completed so far.\n",
    "\n",
    "Once all tasks have completed, we can get some information about the elapsed time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.843275, 10.093617999999998)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.elapsed, ar.serial_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number represents the __actual elapsed__ time for the entire job, while the second number represents the __cumulative time__ spent on all engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine all results with the `ar.result()` method. This is the __list of all results__ returned by the 100 tasks. We use the pi() function to get the final estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[785031, 785106, 785003, 785859, 785476, 785609, 785184, 785558, 785772, 785583, 785485, 785425, 785371, 785218, 785493, 785697, 785261, 785241, 785652, 785031, 785342, 785193, 785997, 785416, 785160, 785200, 785384, 785673, 786332, 785177, 786004, 785397, 785148, 784814, 785427, 785295, 785806, 785606, 785754, 784712, 785112, 785345, 786177, 785246, 783837, 785325, 785505, 785406, 785509, 785384, 784566, 785953, 784803, 784742, 785783, 785409, 785071, 785788, 785560, 785246, 784681, 785738, 786119, 785308, 784338, 784824, 785710, 784670, 785148, 785126, 785297, 785695, 785873, 785949, 785487, 785554, 785245, 785568, 785740, 785092, 785879, 785840, 785214, 785393, 785362, 785951, 785251, 785364, 785610, 785204, 785532, 785204, 785662, 785304, 784837, 785424, 784836, 785776, 785610, 784837]\n"
     ]
    }
   ],
   "source": [
    "print(ar.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14151644"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi(np.sum(ar.result()), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A progress bar with `Ipython.widgets`\n",
    "\n",
    "We can create cool __Ipython widgets__ to visualize the progress of the tasks. The idea is to create a loop polling for the tasks' status at every `0.1` second. An __`IntProgressWidget`__ widget is updated in real-time and shows the progress of the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress, HTML, VBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress_bar(ar):\n",
    "    # We create a progress bar.\n",
    "    progress = IntProgress()\n",
    "    progress.bar_style = 'info'\n",
    "    # The maximum value is the number of tasks.\n",
    "    progress.max = len(ar.msg_ids)\n",
    "    # We display the widget in the output area.\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "    # Repeat every second:\n",
    "    while not ar.ready():\n",
    "        # Update the widget's value with the\n",
    "        # number of tasks that have finished\n",
    "        # so far.\n",
    "        progress.value = ar.progress\n",
    "        label.value = \"{0} / {1}\".format(ar.progress,len(ar.msg_ids))\n",
    "        time.sleep(0.1)\n",
    "    label.value = \"{0} / {1}\".format(ar.progress,len(ar.msg_ids))\n",
    "    progress.value = progress.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9ecd5013854d91897efc06996d5f4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ar = v.map(sample, args)\n",
    "progress_bar(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Lightning fast Python with Numba](http://roman-kh.github.io/numba-2/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
